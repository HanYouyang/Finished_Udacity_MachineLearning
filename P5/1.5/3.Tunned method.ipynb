{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "# read dataset from download file\n",
    "dataname = ['id','diagnosis','radius_mean','texture_mean','perimeter_mean',\n",
    "            'area_mean','smoothness_mean','compactness_mean','concavity_mean',\n",
    "            'concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se',\n",
    "            'texture_se','perimeter_se','area_se','smoothness_se','compactness_se',\n",
    "            'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',\n",
    "            'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n",
    "            'compactness_worst','concavity_worst','concave points_worst','symmetry_worst',\n",
    "            'fractal_dimension_worst']\n",
    "\n",
    "data = pd.read_csv('data.csv', header = 0)\n",
    "#data.shape\n",
    "\n",
    "from IPython.display import display\n",
    "data = data.drop(\"Unnamed: 32\", axis =1 )\n",
    "#display(data.head())\n",
    "\n",
    "#nddata means new data \n",
    "nddata = data\n",
    "d = {'M' : 0, 'B' : 1}\n",
    "nddata['diagnosis'] =nddata['diagnosis'].map(d)\n",
    "#display(nddata.head())\n",
    "\n",
    "#dedata means delete data\n",
    "dedata = data.drop([\"id\"], axis=1)\n",
    "#display(dedata.describe())\n",
    "#display(dedata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# remove rows contain outliers\n",
    "first_change = dedata[['diagnosis','radius_mean','texture_mean','perimeter_mean',\n",
    "            'area_mean','smoothness_mean','compactness_mean','concavity_mean',\n",
    "            'concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se',\n",
    "            'texture_se','perimeter_se','area_se','smoothness_se','compactness_se',\n",
    "            'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',\n",
    "            'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n",
    "            'compactness_worst','concavity_worst','concave points_worst','symmetry_worst',\n",
    "            'fractal_dimension_worst']][(np.abs(stats.zscore(dedata[['diagnosis','radius_mean','texture_mean','perimeter_mean',\n",
    "            'area_mean','smoothness_mean','compactness_mean','concavity_mean',\n",
    "            'concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se',\n",
    "            'texture_se','perimeter_se','area_se','smoothness_se','compactness_se',\n",
    "            'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',\n",
    "            'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n",
    "            'compactness_worst','concavity_worst','concave points_worst','symmetry_worst',\n",
    "            'fractal_dimension_worst']])) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.834244</td>\n",
       "      <td>0.383627</td>\n",
       "      <td>0.822731</td>\n",
       "      <td>0.766613</td>\n",
       "      <td>0.298029</td>\n",
       "      <td>0.283514</td>\n",
       "      <td>0.273787</td>\n",
       "      <td>0.449232</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842053</td>\n",
       "      <td>0.384927</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.798449</td>\n",
       "      <td>0.388053</td>\n",
       "      <td>0.241722</td>\n",
       "      <td>0.284604</td>\n",
       "      <td>0.688634</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.549262</td>\n",
       "      <td>0.795956</td>\n",
       "      <td>0.686872</td>\n",
       "      <td>0.631318</td>\n",
       "      <td>0.672280</td>\n",
       "      <td>0.621928</td>\n",
       "      <td>0.818822</td>\n",
       "      <td>0.656956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771964</td>\n",
       "      <td>0.456573</td>\n",
       "      <td>0.701698</td>\n",
       "      <td>0.687077</td>\n",
       "      <td>0.575923</td>\n",
       "      <td>0.619353</td>\n",
       "      <td>0.530569</td>\n",
       "      <td>0.899667</td>\n",
       "      <td>0.642409</td>\n",
       "      <td>0.411586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.817054</td>\n",
       "      <td>0.220371</td>\n",
       "      <td>0.843043</td>\n",
       "      <td>0.747812</td>\n",
       "      <td>0.506636</td>\n",
       "      <td>0.542627</td>\n",
       "      <td>0.623819</td>\n",
       "      <td>0.667734</td>\n",
       "      <td>0.467589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721125</td>\n",
       "      <td>0.157148</td>\n",
       "      <td>0.699636</td>\n",
       "      <td>0.626657</td>\n",
       "      <td>0.512084</td>\n",
       "      <td>0.270929</td>\n",
       "      <td>0.471198</td>\n",
       "      <td>0.601629</td>\n",
       "      <td>0.250627</td>\n",
       "      <td>0.274981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.335748</td>\n",
       "      <td>0.285102</td>\n",
       "      <td>0.358046</td>\n",
       "      <td>0.216272</td>\n",
       "      <td>0.875318</td>\n",
       "      <td>0.720601</td>\n",
       "      <td>0.497164</td>\n",
       "      <td>0.517862</td>\n",
       "      <td>0.670066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372162</td>\n",
       "      <td>0.396418</td>\n",
       "      <td>0.364217</td>\n",
       "      <td>0.250879</td>\n",
       "      <td>0.892385</td>\n",
       "      <td>0.778723</td>\n",
       "      <td>0.630816</td>\n",
       "      <td>0.644576</td>\n",
       "      <td>0.759097</td>\n",
       "      <td>0.877308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.691817</td>\n",
       "      <td>0.488815</td>\n",
       "      <td>0.699935</td>\n",
       "      <td>0.581199</td>\n",
       "      <td>0.430621</td>\n",
       "      <td>0.428763</td>\n",
       "      <td>0.355072</td>\n",
       "      <td>0.473752</td>\n",
       "      <td>0.456664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737907</td>\n",
       "      <td>0.528557</td>\n",
       "      <td>0.706509</td>\n",
       "      <td>0.640635</td>\n",
       "      <td>0.574099</td>\n",
       "      <td>0.354424</td>\n",
       "      <td>0.445753</td>\n",
       "      <td>0.715291</td>\n",
       "      <td>0.469887</td>\n",
       "      <td>0.362257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.413101</td>\n",
       "      <td>0.529272</td>\n",
       "      <td>0.428492</td>\n",
       "      <td>0.281621</td>\n",
       "      <td>0.755999</td>\n",
       "      <td>0.694288</td>\n",
       "      <td>0.295085</td>\n",
       "      <td>0.383163</td>\n",
       "      <td>0.749454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450642</td>\n",
       "      <td>0.544779</td>\n",
       "      <td>0.413705</td>\n",
       "      <td>0.320949</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.529985</td>\n",
       "      <td>0.315467</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>0.511606</td>\n",
       "      <td>0.759676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.369513</td>\n",
       "      <td>0.576392</td>\n",
       "      <td>0.403564</td>\n",
       "      <td>0.243955</td>\n",
       "      <td>0.868615</td>\n",
       "      <td>0.831595</td>\n",
       "      <td>0.585696</td>\n",
       "      <td>0.598784</td>\n",
       "      <td>0.861617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373149</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.383463</td>\n",
       "      <td>0.249842</td>\n",
       "      <td>0.812130</td>\n",
       "      <td>0.802851</td>\n",
       "      <td>0.634939</td>\n",
       "      <td>0.762680</td>\n",
       "      <td>0.882371</td>\n",
       "      <td>0.659752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.554914</td>\n",
       "      <td>0.643979</td>\n",
       "      <td>0.543902</td>\n",
       "      <td>0.424182</td>\n",
       "      <td>0.262099</td>\n",
       "      <td>0.226342</td>\n",
       "      <td>0.103938</td>\n",
       "      <td>0.212740</td>\n",
       "      <td>0.262928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555775</td>\n",
       "      <td>0.738763</td>\n",
       "      <td>0.504433</td>\n",
       "      <td>0.435026</td>\n",
       "      <td>0.336069</td>\n",
       "      <td>0.191720</td>\n",
       "      <td>0.171869</td>\n",
       "      <td>0.369308</td>\n",
       "      <td>0.433814</td>\n",
       "      <td>0.372881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.540180</td>\n",
       "      <td>0.389338</td>\n",
       "      <td>0.552211</td>\n",
       "      <td>0.413290</td>\n",
       "      <td>0.463735</td>\n",
       "      <td>0.525404</td>\n",
       "      <td>0.313611</td>\n",
       "      <td>0.422919</td>\n",
       "      <td>0.491624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616486</td>\n",
       "      <td>0.515715</td>\n",
       "      <td>0.591725</td>\n",
       "      <td>0.502209</td>\n",
       "      <td>0.532148</td>\n",
       "      <td>0.835868</td>\n",
       "      <td>0.467075</td>\n",
       "      <td>0.670122</td>\n",
       "      <td>0.698557</td>\n",
       "      <td>0.629395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.544478</td>\n",
       "      <td>0.677772</td>\n",
       "      <td>0.553135</td>\n",
       "      <td>0.414392</td>\n",
       "      <td>0.288242</td>\n",
       "      <td>0.386662</td>\n",
       "      <td>0.313106</td>\n",
       "      <td>0.343406</td>\n",
       "      <td>0.495266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439783</td>\n",
       "      <td>0.528557</td>\n",
       "      <td>0.423328</td>\n",
       "      <td>0.311705</td>\n",
       "      <td>0.290470</td>\n",
       "      <td>0.250929</td>\n",
       "      <td>0.273530</td>\n",
       "      <td>0.414291</td>\n",
       "      <td>0.390213</td>\n",
       "      <td>0.099039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.464055</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>0.334068</td>\n",
       "      <td>0.688966</td>\n",
       "      <td>0.670366</td>\n",
       "      <td>0.516383</td>\n",
       "      <td>0.471447</td>\n",
       "      <td>0.827385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470385</td>\n",
       "      <td>0.848597</td>\n",
       "      <td>0.506495</td>\n",
       "      <td>0.341780</td>\n",
       "      <td>0.789330</td>\n",
       "      <td>0.989523</td>\n",
       "      <td>0.827659</td>\n",
       "      <td>0.633839</td>\n",
       "      <td>0.832183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472650</td>\n",
       "      <td>0.495954</td>\n",
       "      <td>0.470409</td>\n",
       "      <td>0.350729</td>\n",
       "      <td>0.484783</td>\n",
       "      <td>0.251746</td>\n",
       "      <td>0.232987</td>\n",
       "      <td>0.336684</td>\n",
       "      <td>0.305171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549852</td>\n",
       "      <td>0.637377</td>\n",
       "      <td>0.501684</td>\n",
       "      <td>0.429615</td>\n",
       "      <td>0.594163</td>\n",
       "      <td>0.242516</td>\n",
       "      <td>0.343268</td>\n",
       "      <td>0.595705</td>\n",
       "      <td>0.459222</td>\n",
       "      <td>0.343031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.561667</td>\n",
       "      <td>0.522132</td>\n",
       "      <td>0.593759</td>\n",
       "      <td>0.424830</td>\n",
       "      <td>0.730527</td>\n",
       "      <td>0.874653</td>\n",
       "      <td>0.542533</td>\n",
       "      <td>0.658131</td>\n",
       "      <td>0.726147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643139</td>\n",
       "      <td>0.657655</td>\n",
       "      <td>0.593787</td>\n",
       "      <td>0.509424</td>\n",
       "      <td>0.890561</td>\n",
       "      <td>0.617448</td>\n",
       "      <td>0.563553</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.671581</td>\n",
       "      <td>0.748292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.787587</td>\n",
       "      <td>0.592099</td>\n",
       "      <td>0.795956</td>\n",
       "      <td>0.723825</td>\n",
       "      <td>0.479957</td>\n",
       "      <td>0.398622</td>\n",
       "      <td>0.465974</td>\n",
       "      <td>0.608067</td>\n",
       "      <td>0.302258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957058</td>\n",
       "      <td>0.637377</td>\n",
       "      <td>0.937453</td>\n",
       "      <td>0.997746</td>\n",
       "      <td>0.637939</td>\n",
       "      <td>0.445538</td>\n",
       "      <td>0.632819</td>\n",
       "      <td>0.884117</td>\n",
       "      <td>0.377353</td>\n",
       "      <td>0.267012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.402664</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>0.403195</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.472986</td>\n",
       "      <td>0.296192</td>\n",
       "      <td>0.209956</td>\n",
       "      <td>0.306082</td>\n",
       "      <td>0.522942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354393</td>\n",
       "      <td>0.244677</td>\n",
       "      <td>0.338786</td>\n",
       "      <td>0.237172</td>\n",
       "      <td>0.572275</td>\n",
       "      <td>0.226960</td>\n",
       "      <td>0.281541</td>\n",
       "      <td>0.476860</td>\n",
       "      <td>0.442911</td>\n",
       "      <td>0.221983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.374424</td>\n",
       "      <td>0.285578</td>\n",
       "      <td>0.386299</td>\n",
       "      <td>0.244084</td>\n",
       "      <td>0.603164</td>\n",
       "      <td>0.514879</td>\n",
       "      <td>0.143919</td>\n",
       "      <td>0.199104</td>\n",
       "      <td>0.582666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324284</td>\n",
       "      <td>0.286245</td>\n",
       "      <td>0.313973</td>\n",
       "      <td>0.200785</td>\n",
       "      <td>0.455540</td>\n",
       "      <td>0.386171</td>\n",
       "      <td>0.222641</td>\n",
       "      <td>0.269641</td>\n",
       "      <td>0.507842</td>\n",
       "      <td>0.338857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.154890</td>\n",
       "      <td>0.129938</td>\n",
       "      <td>0.152802</td>\n",
       "      <td>0.084538</td>\n",
       "      <td>0.534790</td>\n",
       "      <td>0.217874</td>\n",
       "      <td>0.093132</td>\n",
       "      <td>0.132907</td>\n",
       "      <td>0.471959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.123015</td>\n",
       "      <td>0.101175</td>\n",
       "      <td>0.058481</td>\n",
       "      <td>0.466484</td>\n",
       "      <td>0.127750</td>\n",
       "      <td>0.104453</td>\n",
       "      <td>0.230544</td>\n",
       "      <td>0.277604</td>\n",
       "      <td>0.286997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.513168</td>\n",
       "      <td>0.216564</td>\n",
       "      <td>0.542055</td>\n",
       "      <td>0.363630</td>\n",
       "      <td>0.600483</td>\n",
       "      <td>0.928715</td>\n",
       "      <td>0.654379</td>\n",
       "      <td>0.624584</td>\n",
       "      <td>0.986162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500494</td>\n",
       "      <td>0.238594</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.358779</td>\n",
       "      <td>0.526676</td>\n",
       "      <td>0.890631</td>\n",
       "      <td>0.742726</td>\n",
       "      <td>0.885968</td>\n",
       "      <td>0.973024</td>\n",
       "      <td>0.561852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>0.555450</td>\n",
       "      <td>0.611301</td>\n",
       "      <td>0.493420</td>\n",
       "      <td>0.664834</td>\n",
       "      <td>0.604344</td>\n",
       "      <td>0.480466</td>\n",
       "      <td>0.587068</td>\n",
       "      <td>0.603059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914610</td>\n",
       "      <td>0.660358</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.915231</td>\n",
       "      <td>0.905153</td>\n",
       "      <td>0.513477</td>\n",
       "      <td>0.553069</td>\n",
       "      <td>0.775639</td>\n",
       "      <td>0.642409</td>\n",
       "      <td>0.513534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.466511</td>\n",
       "      <td>0.562589</td>\n",
       "      <td>0.495060</td>\n",
       "      <td>0.324992</td>\n",
       "      <td>0.575010</td>\n",
       "      <td>0.800976</td>\n",
       "      <td>0.448960</td>\n",
       "      <td>0.562292</td>\n",
       "      <td>0.790240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478282</td>\n",
       "      <td>0.716120</td>\n",
       "      <td>0.494811</td>\n",
       "      <td>0.320904</td>\n",
       "      <td>0.649795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846612</td>\n",
       "      <td>0.916519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.713917</td>\n",
       "      <td>0.501666</td>\n",
       "      <td>0.723017</td>\n",
       "      <td>0.616207</td>\n",
       "      <td>0.427537</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.469439</td>\n",
       "      <td>0.494942</td>\n",
       "      <td>0.386016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660415</td>\n",
       "      <td>0.515039</td>\n",
       "      <td>0.615094</td>\n",
       "      <td>0.549103</td>\n",
       "      <td>0.479252</td>\n",
       "      <td>0.281564</td>\n",
       "      <td>0.405937</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.243413</td>\n",
       "      <td>0.242474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.510713</td>\n",
       "      <td>0.740600</td>\n",
       "      <td>0.541132</td>\n",
       "      <td>0.381783</td>\n",
       "      <td>0.612549</td>\n",
       "      <td>0.719166</td>\n",
       "      <td>0.530246</td>\n",
       "      <td>0.560243</td>\n",
       "      <td>0.552804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609082</td>\n",
       "      <td>0.834404</td>\n",
       "      <td>0.679703</td>\n",
       "      <td>0.488682</td>\n",
       "      <td>0.755586</td>\n",
       "      <td>0.915394</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.749352</td>\n",
       "      <td>0.772271</td>\n",
       "      <td>0.552998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.650071</td>\n",
       "      <td>0.254165</td>\n",
       "      <td>0.657465</td>\n",
       "      <td>0.526159</td>\n",
       "      <td>0.482102</td>\n",
       "      <td>0.460817</td>\n",
       "      <td>0.311122</td>\n",
       "      <td>0.509155</td>\n",
       "      <td>0.416606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596249</td>\n",
       "      <td>0.253464</td>\n",
       "      <td>0.580727</td>\n",
       "      <td>0.469745</td>\n",
       "      <td>0.403557</td>\n",
       "      <td>0.391885</td>\n",
       "      <td>0.293203</td>\n",
       "      <td>0.539060</td>\n",
       "      <td>0.373588</td>\n",
       "      <td>0.305464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.715145</td>\n",
       "      <td>0.732984</td>\n",
       "      <td>0.747946</td>\n",
       "      <td>0.612318</td>\n",
       "      <td>0.588417</td>\n",
       "      <td>0.810066</td>\n",
       "      <td>0.730624</td>\n",
       "      <td>0.796415</td>\n",
       "      <td>0.739985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751234</td>\n",
       "      <td>0.743156</td>\n",
       "      <td>0.756684</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0.618787</td>\n",
       "      <td>0.621258</td>\n",
       "      <td>0.722464</td>\n",
       "      <td>0.684191</td>\n",
       "      <td>0.589398</td>\n",
       "      <td>0.541108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0.616305</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.637153</td>\n",
       "      <td>0.489984</td>\n",
       "      <td>0.766725</td>\n",
       "      <td>0.623003</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.770166</td>\n",
       "      <td>0.787327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639191</td>\n",
       "      <td>0.678270</td>\n",
       "      <td>0.588975</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.749202</td>\n",
       "      <td>0.510461</td>\n",
       "      <td>0.658264</td>\n",
       "      <td>0.683821</td>\n",
       "      <td>0.616374</td>\n",
       "      <td>0.376676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0.754436</td>\n",
       "      <td>0.797715</td>\n",
       "      <td>0.776567</td>\n",
       "      <td>0.660292</td>\n",
       "      <td>0.422309</td>\n",
       "      <td>0.729691</td>\n",
       "      <td>0.522054</td>\n",
       "      <td>0.486108</td>\n",
       "      <td>0.499636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800592</td>\n",
       "      <td>0.638053</td>\n",
       "      <td>0.762870</td>\n",
       "      <td>0.733971</td>\n",
       "      <td>0.635203</td>\n",
       "      <td>0.991587</td>\n",
       "      <td>0.717517</td>\n",
       "      <td>0.660866</td>\n",
       "      <td>0.660916</td>\n",
       "      <td>0.724260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0.561667</td>\n",
       "      <td>0.388862</td>\n",
       "      <td>0.583603</td>\n",
       "      <td>0.430276</td>\n",
       "      <td>0.556241</td>\n",
       "      <td>0.653143</td>\n",
       "      <td>0.426591</td>\n",
       "      <td>0.496287</td>\n",
       "      <td>0.605244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606120</td>\n",
       "      <td>0.515039</td>\n",
       "      <td>0.565606</td>\n",
       "      <td>0.485075</td>\n",
       "      <td>0.577747</td>\n",
       "      <td>0.866821</td>\n",
       "      <td>0.621275</td>\n",
       "      <td>0.690115</td>\n",
       "      <td>0.848494</td>\n",
       "      <td>0.863395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.446252</td>\n",
       "      <td>0.571633</td>\n",
       "      <td>0.460161</td>\n",
       "      <td>0.317342</td>\n",
       "      <td>0.478885</td>\n",
       "      <td>0.432590</td>\n",
       "      <td>0.415564</td>\n",
       "      <td>0.358387</td>\n",
       "      <td>0.522942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392892</td>\n",
       "      <td>0.619804</td>\n",
       "      <td>0.452196</td>\n",
       "      <td>0.277031</td>\n",
       "      <td>0.577747</td>\n",
       "      <td>0.618242</td>\n",
       "      <td>0.610908</td>\n",
       "      <td>0.535728</td>\n",
       "      <td>0.635508</td>\n",
       "      <td>0.586390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0.371355</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>0.358416</td>\n",
       "      <td>0.246548</td>\n",
       "      <td>0.366269</td>\n",
       "      <td>0.087456</td>\n",
       "      <td>0.080718</td>\n",
       "      <td>0.187132</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265054</td>\n",
       "      <td>0.364650</td>\n",
       "      <td>0.234037</td>\n",
       "      <td>0.162639</td>\n",
       "      <td>0.143730</td>\n",
       "      <td>0.018842</td>\n",
       "      <td>0.056933</td>\n",
       "      <td>0.185598</td>\n",
       "      <td>0.132371</td>\n",
       "      <td>0.084113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0.491682</td>\n",
       "      <td>0.737268</td>\n",
       "      <td>0.477795</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.420432</td>\n",
       "      <td>0.152761</td>\n",
       "      <td>0.075551</td>\n",
       "      <td>0.185595</td>\n",
       "      <td>0.289876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348470</td>\n",
       "      <td>0.445421</td>\n",
       "      <td>0.310193</td>\n",
       "      <td>0.231581</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.028248</td>\n",
       "      <td>0.107331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0</td>\n",
       "      <td>0.828105</td>\n",
       "      <td>0.521656</td>\n",
       "      <td>0.839350</td>\n",
       "      <td>0.749109</td>\n",
       "      <td>0.389462</td>\n",
       "      <td>0.535451</td>\n",
       "      <td>0.479836</td>\n",
       "      <td>0.649808</td>\n",
       "      <td>0.727604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755183</td>\n",
       "      <td>0.511321</td>\n",
       "      <td>0.698261</td>\n",
       "      <td>0.658220</td>\n",
       "      <td>0.259462</td>\n",
       "      <td>0.347757</td>\n",
       "      <td>0.364236</td>\n",
       "      <td>0.597186</td>\n",
       "      <td>0.519134</td>\n",
       "      <td>0.111561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1</td>\n",
       "      <td>0.244275</td>\n",
       "      <td>0.376487</td>\n",
       "      <td>0.249284</td>\n",
       "      <td>0.143987</td>\n",
       "      <td>0.460652</td>\n",
       "      <td>0.373840</td>\n",
       "      <td>0.165816</td>\n",
       "      <td>0.178489</td>\n",
       "      <td>0.329206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182132</td>\n",
       "      <td>0.489692</td>\n",
       "      <td>0.178844</td>\n",
       "      <td>0.100234</td>\n",
       "      <td>0.561332</td>\n",
       "      <td>0.343947</td>\n",
       "      <td>0.250088</td>\n",
       "      <td>0.365087</td>\n",
       "      <td>0.227102</td>\n",
       "      <td>0.350873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0</td>\n",
       "      <td>0.833016</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.867972</td>\n",
       "      <td>0.754943</td>\n",
       "      <td>0.564285</td>\n",
       "      <td>0.739259</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.846351</td>\n",
       "      <td>0.699199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807996</td>\n",
       "      <td>0.454883</td>\n",
       "      <td>0.754622</td>\n",
       "      <td>0.732167</td>\n",
       "      <td>0.415413</td>\n",
       "      <td>0.443157</td>\n",
       "      <td>0.522205</td>\n",
       "      <td>0.795261</td>\n",
       "      <td>0.474279</td>\n",
       "      <td>0.261194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0</td>\n",
       "      <td>0.447480</td>\n",
       "      <td>0.611138</td>\n",
       "      <td>0.461453</td>\n",
       "      <td>0.315267</td>\n",
       "      <td>0.553559</td>\n",
       "      <td>0.459382</td>\n",
       "      <td>0.460933</td>\n",
       "      <td>0.393022</td>\n",
       "      <td>0.552804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363277</td>\n",
       "      <td>0.751943</td>\n",
       "      <td>0.370403</td>\n",
       "      <td>0.244882</td>\n",
       "      <td>0.517556</td>\n",
       "      <td>0.379345</td>\n",
       "      <td>0.498763</td>\n",
       "      <td>0.504258</td>\n",
       "      <td>0.355395</td>\n",
       "      <td>0.360106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1</td>\n",
       "      <td>0.289091</td>\n",
       "      <td>0.701095</td>\n",
       "      <td>0.300803</td>\n",
       "      <td>0.170438</td>\n",
       "      <td>0.819011</td>\n",
       "      <td>0.649794</td>\n",
       "      <td>0.142250</td>\n",
       "      <td>0.290077</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249260</td>\n",
       "      <td>0.681649</td>\n",
       "      <td>0.245446</td>\n",
       "      <td>0.136396</td>\n",
       "      <td>0.871409</td>\n",
       "      <td>0.461570</td>\n",
       "      <td>0.164330</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.388331</td>\n",
       "      <td>0.564887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.751071</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.022885</td>\n",
       "      <td>0.247620</td>\n",
       "      <td>0.140656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056614</td>\n",
       "      <td>0.638729</td>\n",
       "      <td>0.046464</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.404469</td>\n",
       "      <td>0.077907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468319</td>\n",
       "      <td>0.560840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1</td>\n",
       "      <td>0.043588</td>\n",
       "      <td>0.748691</td>\n",
       "      <td>0.042009</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.324038</td>\n",
       "      <td>0.480911</td>\n",
       "      <td>0.291493</td>\n",
       "      <td>0.087324</td>\n",
       "      <td>0.633649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>0.671511</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.714546</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.399694</td>\n",
       "      <td>0.185117</td>\n",
       "      <td>0.384253</td>\n",
       "      <td>0.652163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "      <td>0.279882</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>0.168169</td>\n",
       "      <td>0.500469</td>\n",
       "      <td>0.443115</td>\n",
       "      <td>0.212256</td>\n",
       "      <td>0.166069</td>\n",
       "      <td>0.474144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213722</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.194996</td>\n",
       "      <td>0.122915</td>\n",
       "      <td>0.485636</td>\n",
       "      <td>0.281723</td>\n",
       "      <td>0.211686</td>\n",
       "      <td>0.256127</td>\n",
       "      <td>0.239649</td>\n",
       "      <td>0.332659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1</td>\n",
       "      <td>0.459758</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.480288</td>\n",
       "      <td>0.332512</td>\n",
       "      <td>0.346695</td>\n",
       "      <td>0.495742</td>\n",
       "      <td>0.317895</td>\n",
       "      <td>0.249040</td>\n",
       "      <td>0.513474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409181</td>\n",
       "      <td>0.666103</td>\n",
       "      <td>0.433638</td>\n",
       "      <td>0.281225</td>\n",
       "      <td>0.481076</td>\n",
       "      <td>0.612527</td>\n",
       "      <td>0.475910</td>\n",
       "      <td>0.446131</td>\n",
       "      <td>0.508783</td>\n",
       "      <td>0.597774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1</td>\n",
       "      <td>0.476334</td>\n",
       "      <td>0.747739</td>\n",
       "      <td>0.470040</td>\n",
       "      <td>0.340421</td>\n",
       "      <td>0.271350</td>\n",
       "      <td>0.252416</td>\n",
       "      <td>0.129332</td>\n",
       "      <td>0.193790</td>\n",
       "      <td>0.490168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423495</td>\n",
       "      <td>0.685029</td>\n",
       "      <td>0.391711</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.225718</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.189775</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.362923</td>\n",
       "      <td>0.183658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1</td>\n",
       "      <td>0.382405</td>\n",
       "      <td>0.873394</td>\n",
       "      <td>0.379374</td>\n",
       "      <td>0.256013</td>\n",
       "      <td>0.324440</td>\n",
       "      <td>0.236293</td>\n",
       "      <td>0.094108</td>\n",
       "      <td>0.209667</td>\n",
       "      <td>0.335761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317868</td>\n",
       "      <td>0.849949</td>\n",
       "      <td>0.289161</td>\n",
       "      <td>0.200379</td>\n",
       "      <td>0.236662</td>\n",
       "      <td>0.164735</td>\n",
       "      <td>0.125103</td>\n",
       "      <td>0.294632</td>\n",
       "      <td>0.284818</td>\n",
       "      <td>0.118771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1</td>\n",
       "      <td>0.422923</td>\n",
       "      <td>0.523084</td>\n",
       "      <td>0.424522</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.446038</td>\n",
       "      <td>0.394316</td>\n",
       "      <td>0.116194</td>\n",
       "      <td>0.151665</td>\n",
       "      <td>0.329934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351431</td>\n",
       "      <td>0.430213</td>\n",
       "      <td>0.335143</td>\n",
       "      <td>0.226982</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.268866</td>\n",
       "      <td>0.162210</td>\n",
       "      <td>0.253425</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>0.377941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1</td>\n",
       "      <td>0.407576</td>\n",
       "      <td>0.643503</td>\n",
       "      <td>0.400702</td>\n",
       "      <td>0.278574</td>\n",
       "      <td>0.401528</td>\n",
       "      <td>0.230074</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>0.156402</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366239</td>\n",
       "      <td>0.576884</td>\n",
       "      <td>0.324215</td>\n",
       "      <td>0.245559</td>\n",
       "      <td>0.367989</td>\n",
       "      <td>0.186323</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0.265605</td>\n",
       "      <td>0.337829</td>\n",
       "      <td>0.183279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1</td>\n",
       "      <td>0.204985</td>\n",
       "      <td>0.316040</td>\n",
       "      <td>0.198689</td>\n",
       "      <td>0.117601</td>\n",
       "      <td>0.426733</td>\n",
       "      <td>0.146206</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>0.035179</td>\n",
       "      <td>0.522942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163870</td>\n",
       "      <td>0.329503</td>\n",
       "      <td>0.142347</td>\n",
       "      <td>0.090044</td>\n",
       "      <td>0.430917</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.088153</td>\n",
       "      <td>0.350063</td>\n",
       "      <td>0.239691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1</td>\n",
       "      <td>0.201301</td>\n",
       "      <td>0.326987</td>\n",
       "      <td>0.203675</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.352058</td>\n",
       "      <td>0.293178</td>\n",
       "      <td>0.137303</td>\n",
       "      <td>0.156082</td>\n",
       "      <td>0.365623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143139</td>\n",
       "      <td>0.338628</td>\n",
       "      <td>0.142072</td>\n",
       "      <td>0.077645</td>\n",
       "      <td>0.591427</td>\n",
       "      <td>0.302041</td>\n",
       "      <td>0.210037</td>\n",
       "      <td>0.308515</td>\n",
       "      <td>0.353199</td>\n",
       "      <td>0.502783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1</td>\n",
       "      <td>0.165879</td>\n",
       "      <td>0.458353</td>\n",
       "      <td>0.159357</td>\n",
       "      <td>0.092188</td>\n",
       "      <td>0.300308</td>\n",
       "      <td>0.147928</td>\n",
       "      <td>0.073629</td>\n",
       "      <td>0.061556</td>\n",
       "      <td>0.300801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148075</td>\n",
       "      <td>0.458601</td>\n",
       "      <td>0.128462</td>\n",
       "      <td>0.080711</td>\n",
       "      <td>0.352485</td>\n",
       "      <td>0.097051</td>\n",
       "      <td>0.110143</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>0.309598</td>\n",
       "      <td>0.305591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1</td>\n",
       "      <td>0.235681</td>\n",
       "      <td>0.690148</td>\n",
       "      <td>0.231742</td>\n",
       "      <td>0.141394</td>\n",
       "      <td>0.260223</td>\n",
       "      <td>0.223137</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.052241</td>\n",
       "      <td>0.589221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251728</td>\n",
       "      <td>0.656641</td>\n",
       "      <td>0.230188</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.204737</td>\n",
       "      <td>0.072965</td>\n",
       "      <td>0.120844</td>\n",
       "      <td>0.468632</td>\n",
       "      <td>0.268404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>0.560209</td>\n",
       "      <td>0.228234</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.158198</td>\n",
       "      <td>0.109511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184107</td>\n",
       "      <td>0.430889</td>\n",
       "      <td>0.162692</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.171911</td>\n",
       "      <td>0.062161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280113</td>\n",
       "      <td>0.137617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>1</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.605902</td>\n",
       "      <td>0.255747</td>\n",
       "      <td>0.152285</td>\n",
       "      <td>0.444430</td>\n",
       "      <td>0.299302</td>\n",
       "      <td>0.151985</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.628551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201876</td>\n",
       "      <td>0.548834</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>0.113356</td>\n",
       "      <td>0.250342</td>\n",
       "      <td>0.228388</td>\n",
       "      <td>0.184238</td>\n",
       "      <td>0.237431</td>\n",
       "      <td>0.503137</td>\n",
       "      <td>0.319757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1</td>\n",
       "      <td>0.355393</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.346782</td>\n",
       "      <td>0.236240</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.109846</td>\n",
       "      <td>0.062917</td>\n",
       "      <td>0.095967</td>\n",
       "      <td>0.270940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293189</td>\n",
       "      <td>0.810409</td>\n",
       "      <td>0.259056</td>\n",
       "      <td>0.184642</td>\n",
       "      <td>0.384405</td>\n",
       "      <td>0.114416</td>\n",
       "      <td>0.101932</td>\n",
       "      <td>0.240578</td>\n",
       "      <td>0.264115</td>\n",
       "      <td>0.123956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1</td>\n",
       "      <td>0.144392</td>\n",
       "      <td>0.582104</td>\n",
       "      <td>0.140523</td>\n",
       "      <td>0.078120</td>\n",
       "      <td>0.400724</td>\n",
       "      <td>0.175438</td>\n",
       "      <td>0.125898</td>\n",
       "      <td>0.082074</td>\n",
       "      <td>0.382374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094521</td>\n",
       "      <td>0.440351</td>\n",
       "      <td>0.085573</td>\n",
       "      <td>0.049869</td>\n",
       "      <td>0.264934</td>\n",
       "      <td>0.077241</td>\n",
       "      <td>0.094157</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>0.272898</td>\n",
       "      <td>0.238932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1</td>\n",
       "      <td>0.362146</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>0.250972</td>\n",
       "      <td>0.185915</td>\n",
       "      <td>0.195180</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.290605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294176</td>\n",
       "      <td>0.801622</td>\n",
       "      <td>0.264142</td>\n",
       "      <td>0.185093</td>\n",
       "      <td>0.378021</td>\n",
       "      <td>0.202673</td>\n",
       "      <td>0.287313</td>\n",
       "      <td>0.240392</td>\n",
       "      <td>0.253137</td>\n",
       "      <td>0.219833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1</td>\n",
       "      <td>0.203143</td>\n",
       "      <td>0.851975</td>\n",
       "      <td>0.202013</td>\n",
       "      <td>0.115332</td>\n",
       "      <td>0.372570</td>\n",
       "      <td>0.273658</td>\n",
       "      <td>0.189004</td>\n",
       "      <td>0.175288</td>\n",
       "      <td>0.310269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143633</td>\n",
       "      <td>0.773572</td>\n",
       "      <td>0.131693</td>\n",
       "      <td>0.077735</td>\n",
       "      <td>0.521204</td>\n",
       "      <td>0.216959</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>0.207340</td>\n",
       "      <td>0.351505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "      <td>0.195162</td>\n",
       "      <td>0.470252</td>\n",
       "      <td>0.193334</td>\n",
       "      <td>0.109044</td>\n",
       "      <td>0.506636</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.071447</td>\n",
       "      <td>0.454479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134255</td>\n",
       "      <td>0.367016</td>\n",
       "      <td>0.120077</td>\n",
       "      <td>0.073090</td>\n",
       "      <td>0.412677</td>\n",
       "      <td>0.136004</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.082636</td>\n",
       "      <td>0.218632</td>\n",
       "      <td>0.156590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "      <td>0.467125</td>\n",
       "      <td>0.617325</td>\n",
       "      <td>0.485643</td>\n",
       "      <td>0.332966</td>\n",
       "      <td>0.297895</td>\n",
       "      <td>0.543584</td>\n",
       "      <td>0.324197</td>\n",
       "      <td>0.239181</td>\n",
       "      <td>0.209031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372655</td>\n",
       "      <td>0.515377</td>\n",
       "      <td>0.381401</td>\n",
       "      <td>0.247227</td>\n",
       "      <td>0.194710</td>\n",
       "      <td>0.448871</td>\n",
       "      <td>0.431382</td>\n",
       "      <td>0.409108</td>\n",
       "      <td>0.217378</td>\n",
       "      <td>0.316216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1</td>\n",
       "      <td>0.433974</td>\n",
       "      <td>0.830081</td>\n",
       "      <td>0.439387</td>\n",
       "      <td>0.296207</td>\n",
       "      <td>0.493096</td>\n",
       "      <td>0.445986</td>\n",
       "      <td>0.140580</td>\n",
       "      <td>0.275544</td>\n",
       "      <td>0.269483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363771</td>\n",
       "      <td>0.714769</td>\n",
       "      <td>0.342223</td>\n",
       "      <td>0.235143</td>\n",
       "      <td>0.390789</td>\n",
       "      <td>0.304899</td>\n",
       "      <td>0.156202</td>\n",
       "      <td>0.388004</td>\n",
       "      <td>0.214868</td>\n",
       "      <td>0.356312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>0.855731</td>\n",
       "      <td>0.732032</td>\n",
       "      <td>0.915982</td>\n",
       "      <td>0.780227</td>\n",
       "      <td>0.635340</td>\n",
       "      <td>0.977036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.715222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807502</td>\n",
       "      <td>0.587699</td>\n",
       "      <td>0.884528</td>\n",
       "      <td>0.736676</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.609988</td>\n",
       "      <td>0.777359</td>\n",
       "      <td>0.941133</td>\n",
       "      <td>0.427854</td>\n",
       "      <td>0.552618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>0.895021</td>\n",
       "      <td>0.603522</td>\n",
       "      <td>0.906749</td>\n",
       "      <td>0.865802</td>\n",
       "      <td>0.650087</td>\n",
       "      <td>0.461774</td>\n",
       "      <td>0.768431</td>\n",
       "      <td>0.889245</td>\n",
       "      <td>0.407138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864758</td>\n",
       "      <td>0.485975</td>\n",
       "      <td>0.795175</td>\n",
       "      <td>0.830463</td>\n",
       "      <td>0.544916</td>\n",
       "      <td>0.280930</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.820437</td>\n",
       "      <td>0.155270</td>\n",
       "      <td>0.203769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>0.807232</td>\n",
       "      <td>0.882437</td>\n",
       "      <td>0.807035</td>\n",
       "      <td>0.724473</td>\n",
       "      <td>0.473120</td>\n",
       "      <td>0.401971</td>\n",
       "      <td>0.453686</td>\n",
       "      <td>0.626825</td>\n",
       "      <td>0.426074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777887</td>\n",
       "      <td>0.886448</td>\n",
       "      <td>0.718881</td>\n",
       "      <td>0.696997</td>\n",
       "      <td>0.322389</td>\n",
       "      <td>0.250611</td>\n",
       "      <td>0.378725</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.315872</td>\n",
       "      <td>0.143309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>0.590521</td>\n",
       "      <td>0.874346</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>0.463274</td>\n",
       "      <td>0.295482</td>\n",
       "      <td>0.396708</td>\n",
       "      <td>0.291462</td>\n",
       "      <td>0.339437</td>\n",
       "      <td>0.308084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545410</td>\n",
       "      <td>0.746874</td>\n",
       "      <td>0.524366</td>\n",
       "      <td>0.423302</td>\n",
       "      <td>0.297766</td>\n",
       "      <td>0.436649</td>\n",
       "      <td>0.400872</td>\n",
       "      <td>0.524991</td>\n",
       "      <td>0.204831</td>\n",
       "      <td>0.292942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "1            0     0.834244      0.383627        0.822731   0.766613   \n",
       "2            0     0.780220      0.549262        0.795956   0.686872   \n",
       "4            0     0.817054      0.220371        0.843043   0.747812   \n",
       "5            0     0.335748      0.285102        0.358046   0.216272   \n",
       "6            0     0.691817      0.488815        0.699935   0.581199   \n",
       "7            0     0.413101      0.529272        0.428492   0.281621   \n",
       "8            0     0.369513      0.576392        0.403564   0.243955   \n",
       "10           0     0.554914      0.643979        0.543902   0.424182   \n",
       "11           0     0.540180      0.389338        0.552211   0.413290   \n",
       "13           0     0.544478      0.677772        0.553135   0.414392   \n",
       "15           0     0.464055      0.848644        0.488782   0.334068   \n",
       "16           0     0.472650      0.495954        0.470409   0.350729   \n",
       "17           0     0.561667      0.522132        0.593759   0.424830   \n",
       "18           0     0.787587      0.592099        0.795956   0.723825   \n",
       "19           1     0.402664      0.221323        0.403195   0.274100   \n",
       "20           1     0.374424      0.285578        0.386299   0.244084   \n",
       "21           1     0.154890      0.129938        0.152802   0.084538   \n",
       "22           0     0.513168      0.216564        0.542055   0.363630   \n",
       "24           0     0.593591      0.555450        0.611301   0.493420   \n",
       "26           0     0.466511      0.562589        0.495060   0.324992   \n",
       "27           0     0.713917      0.501666        0.723017   0.616207   \n",
       "28           0     0.510713      0.740600        0.541132   0.381783   \n",
       "29           0     0.650071      0.254165        0.657465   0.526159   \n",
       "30           0     0.715145      0.732984        0.747946   0.612318   \n",
       "32           0     0.616305      0.679200        0.637153   0.489984   \n",
       "33           0     0.754436      0.797715        0.776567   0.660292   \n",
       "34           0     0.561667      0.388862        0.583603   0.430276   \n",
       "36           0     0.446252      0.571633        0.460161   0.317342   \n",
       "37           1     0.371355      0.414564        0.358416   0.246548   \n",
       "38           0     0.491682      0.737268        0.477795   0.360000   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "533          0     0.828105      0.521656        0.839350   0.749109   \n",
       "534          1     0.244275      0.376487        0.249284   0.143987   \n",
       "535          0     0.833016      0.530700        0.867972   0.754943   \n",
       "536          0     0.447480      0.611138        0.461453   0.315267   \n",
       "537          1     0.289091      0.701095        0.300803   0.170438   \n",
       "538          1     0.045921      0.751071        0.038685   0.022885   \n",
       "539          1     0.043588      0.748691        0.042009   0.017439   \n",
       "540          1     0.279882      0.225131        0.284923   0.168169   \n",
       "541          1     0.459758      0.727273        0.480288   0.332512   \n",
       "542          1     0.476334      0.747739        0.470040   0.340421   \n",
       "543          1     0.382405      0.873394        0.379374   0.256013   \n",
       "544          1     0.422923      0.523084        0.424522   0.286094   \n",
       "545          1     0.407576      0.643503        0.400702   0.278574   \n",
       "546          1     0.204985      0.316040        0.198689   0.117601   \n",
       "547          1     0.201301      0.326987        0.203675   0.114943   \n",
       "548          1     0.165879      0.458353        0.159357   0.092188   \n",
       "549          1     0.235681      0.690148        0.231742   0.141394   \n",
       "550          1     0.238136      0.560209        0.228234   0.140681   \n",
       "551          1     0.254712      0.605902        0.255747   0.152285   \n",
       "552          1     0.355393      0.938601        0.346782   0.236240   \n",
       "553          1     0.144392      0.582104        0.140523   0.078120   \n",
       "554          1     0.362146      0.914327        0.357400   0.240389   \n",
       "555          1     0.203143      0.851975        0.202013   0.115332   \n",
       "556          1     0.195162      0.470252        0.193334   0.109044   \n",
       "558          1     0.467125      0.617325        0.485643   0.332966   \n",
       "560          1     0.433974      0.830081        0.439387   0.296207   \n",
       "563          0     0.855731      0.732032        0.915982   0.780227   \n",
       "564          0     0.895021      0.603522        0.906749   0.865802   \n",
       "565          0     0.807232      0.882437        0.807035   0.724473   \n",
       "566          0     0.590521      0.874346        0.595605   0.463274   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "1           0.298029          0.283514        0.273787             0.449232   \n",
       "2           0.631318          0.672280        0.621928             0.818822   \n",
       "4           0.506636          0.542627        0.623819             0.667734   \n",
       "5           0.875318          0.720601        0.497164             0.517862   \n",
       "6           0.430621          0.428763        0.355072             0.473752   \n",
       "7           0.755999          0.694288        0.295085             0.383163   \n",
       "8           0.868615          0.831595        0.585696             0.598784   \n",
       "10          0.262099          0.226342        0.103938             0.212740   \n",
       "11          0.463735          0.525404        0.313611             0.422919   \n",
       "13          0.288242          0.386662        0.313106             0.343406   \n",
       "15          0.688966          0.670366        0.516383             0.471447   \n",
       "16          0.484783          0.251746        0.232987             0.336684   \n",
       "17          0.730527          0.874653        0.542533             0.658131   \n",
       "18          0.479957          0.398622        0.465974             0.608067   \n",
       "19          0.472986          0.296192        0.209956             0.306082   \n",
       "20          0.603164          0.514879        0.143919             0.199104   \n",
       "21          0.534790          0.217874        0.093132             0.132907   \n",
       "22          0.600483          0.928715        0.654379             0.624584   \n",
       "24          0.664834          0.604344        0.480466             0.587068   \n",
       "26          0.575010          0.800976        0.448960             0.562292   \n",
       "27          0.427537          0.417281        0.469439             0.494942   \n",
       "28          0.612549          0.719166        0.530246             0.560243   \n",
       "29          0.482102          0.460817        0.311122             0.509155   \n",
       "30          0.588417          0.810066        0.730624             0.796415   \n",
       "32          0.766725          0.623003        0.761500             0.770166   \n",
       "33          0.422309          0.729691        0.522054             0.486108   \n",
       "34          0.556241          0.653143        0.426591             0.496287   \n",
       "36          0.478885          0.432590        0.415564             0.358387   \n",
       "37          0.366269          0.087456        0.080718             0.187132   \n",
       "38          0.420432          0.152761        0.075551             0.185595   \n",
       "..               ...               ...             ...                  ...   \n",
       "533         0.389462          0.535451        0.479836             0.649808   \n",
       "534         0.460652          0.373840        0.165816             0.178489   \n",
       "535         0.564285          0.739259        0.656900             0.846351   \n",
       "536         0.553559          0.459382        0.460933             0.393022   \n",
       "537         0.819011          0.649794        0.142250             0.290077   \n",
       "538         0.247620          0.140656        0.000000             0.000000   \n",
       "539         0.324038          0.480911        0.291493             0.087324   \n",
       "540         0.500469          0.443115        0.212256             0.166069   \n",
       "541         0.346695          0.495742        0.317895             0.249040   \n",
       "542         0.271350          0.252416        0.129332             0.193790   \n",
       "543         0.324440          0.236293        0.094108             0.209667   \n",
       "544         0.446038          0.394316        0.116194             0.151665   \n",
       "545         0.401528          0.230074        0.093699             0.156402   \n",
       "546         0.426733          0.146206        0.031884             0.035179   \n",
       "547         0.352058          0.293178        0.137303             0.156082   \n",
       "548         0.300308          0.147928        0.073629             0.061556   \n",
       "549         0.260223          0.223137        0.048771             0.052241   \n",
       "550         0.158198          0.109511        0.000000             0.000000   \n",
       "551         0.444430          0.299302        0.151985             0.144494   \n",
       "552         0.271484          0.109846        0.062917             0.095967   \n",
       "553         0.400724          0.175438        0.125898             0.082074   \n",
       "554         0.250972          0.185915        0.195180             0.150000   \n",
       "555         0.372570          0.273658        0.189004             0.175288   \n",
       "556         0.506636          0.266290        0.015832             0.071447   \n",
       "558         0.297895          0.543584        0.324197             0.239181   \n",
       "560         0.493096          0.445986        0.140580             0.275544   \n",
       "563         0.635340          0.977036        1.000000             0.943662   \n",
       "564         0.650087          0.461774        0.768431             0.889245   \n",
       "565         0.473120          0.401971        0.453686             0.626825   \n",
       "566         0.295482          0.396708        0.291462             0.339437   \n",
       "\n",
       "     symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "1         0.469774           ...                 0.842053       0.384927   \n",
       "2         0.656956           ...                 0.771964       0.456573   \n",
       "4         0.467589           ...                 0.721125       0.157148   \n",
       "5         0.670066           ...                 0.372162       0.396418   \n",
       "6         0.456664           ...                 0.737907       0.528557   \n",
       "7         0.749454           ...                 0.450642       0.544779   \n",
       "8         0.861617           ...                 0.373149       0.632308   \n",
       "10        0.262928           ...                 0.555775       0.738763   \n",
       "11        0.491624           ...                 0.616486       0.515715   \n",
       "13        0.495266           ...                 0.439783       0.528557   \n",
       "15        0.827385           ...                 0.470385       0.848597   \n",
       "16        0.305171           ...                 0.549852       0.637377   \n",
       "17        0.726147           ...                 0.643139       0.657655   \n",
       "18        0.302258           ...                 0.957058       0.637377   \n",
       "19        0.522942           ...                 0.354393       0.244677   \n",
       "20        0.582666           ...                 0.324284       0.286245   \n",
       "21        0.471959           ...                 0.113524       0.123015   \n",
       "22        0.986162           ...                 0.500494       0.238594   \n",
       "24        0.603059           ...                 0.914610       0.660358   \n",
       "26        0.790240           ...                 0.478282       0.716120   \n",
       "27        0.386016           ...                 0.660415       0.515039   \n",
       "28        0.552804           ...                 0.609082       0.834404   \n",
       "29        0.416606           ...                 0.596249       0.253464   \n",
       "30        0.739985           ...                 0.751234       0.743156   \n",
       "32        0.787327           ...                 0.639191       0.678270   \n",
       "33        0.499636           ...                 0.800592       0.638053   \n",
       "34        0.605244           ...                 0.606120       0.515039   \n",
       "36        0.522942           ...                 0.392892       0.619804   \n",
       "37        0.218500           ...                 0.265054       0.364650   \n",
       "38        0.289876           ...                 0.348470       0.445421   \n",
       "..             ...           ...                      ...            ...   \n",
       "533       0.727604           ...                 0.755183       0.511321   \n",
       "534       0.329206           ...                 0.182132       0.489692   \n",
       "535       0.699199           ...                 0.807996       0.454883   \n",
       "536       0.552804           ...                 0.363277       0.751943   \n",
       "537       0.702112           ...                 0.249260       0.681649   \n",
       "538       0.512017           ...                 0.056614       0.638729   \n",
       "539       0.633649           ...                 0.036920       0.671511   \n",
       "540       0.474144           ...                 0.213722       0.258871   \n",
       "541       0.513474           ...                 0.409181       0.666103   \n",
       "542       0.490168           ...                 0.423495       0.685029   \n",
       "543       0.335761           ...                 0.317868       0.849949   \n",
       "544       0.329934           ...                 0.351431       0.430213   \n",
       "545       0.361981           ...                 0.366239       0.576884   \n",
       "546       0.522942           ...                 0.163870       0.329503   \n",
       "547       0.365623           ...                 0.143139       0.338628   \n",
       "548       0.300801           ...                 0.148075       0.458601   \n",
       "549       0.589221           ...                 0.251728       0.656641   \n",
       "550       0.359796           ...                 0.184107       0.430889   \n",
       "551       0.628551           ...                 0.201876       0.548834   \n",
       "552       0.270940           ...                 0.293189       0.810409   \n",
       "553       0.382374           ...                 0.094521       0.440351   \n",
       "554       0.290605           ...                 0.294176       0.801622   \n",
       "555       0.310269           ...                 0.143633       0.773572   \n",
       "556       0.454479           ...                 0.134255       0.367016   \n",
       "558       0.209031           ...                 0.372655       0.515377   \n",
       "560       0.269483           ...                 0.363771       0.714769   \n",
       "563       0.715222           ...                 0.807502       0.587699   \n",
       "564       0.407138           ...                 0.864758       0.485975   \n",
       "565       0.426074           ...                 0.777887       0.886448   \n",
       "566       0.308084           ...                 0.545410       0.746874   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "1           0.745000    0.798449          0.388053           0.241722   \n",
       "2           0.701698    0.687077          0.575923           0.619353   \n",
       "4           0.699636    0.626657          0.512084           0.270929   \n",
       "5           0.364217    0.250879          0.892385           0.778723   \n",
       "6           0.706509    0.640635          0.574099           0.354424   \n",
       "7           0.413705    0.320949          0.767442           0.529985   \n",
       "8           0.383463    0.249842          0.812130           0.802851   \n",
       "10          0.504433    0.435026          0.336069           0.191720   \n",
       "11          0.591725    0.502209          0.532148           0.835868   \n",
       "13          0.423328    0.311705          0.290470           0.250929   \n",
       "15          0.506495    0.341780          0.789330           0.989523   \n",
       "16          0.501684    0.429615          0.594163           0.242516   \n",
       "17          0.593787    0.509424          0.890561           0.617448   \n",
       "18          0.937453    0.997746          0.637939           0.445538   \n",
       "19          0.338786    0.237172          0.572275           0.226960   \n",
       "20          0.313973    0.200785          0.455540           0.386171   \n",
       "21          0.101175    0.058481          0.466484           0.127750   \n",
       "22          0.513369    0.358779          0.526676           0.890631   \n",
       "24          0.870094    0.915231          0.905153           0.513477   \n",
       "26          0.494811    0.320904          0.649795           1.000000   \n",
       "27          0.615094    0.549103          0.479252           0.281564   \n",
       "28          0.679703    0.488682          0.755586           0.915394   \n",
       "29          0.580727    0.469745          0.403557           0.391885   \n",
       "30          0.756684    0.669492          0.618787           0.621258   \n",
       "32          0.588975    0.522500          0.749202           0.510461   \n",
       "33          0.762870    0.733971          0.635203           0.991587   \n",
       "34          0.565606    0.485075          0.577747           0.866821   \n",
       "36          0.452196    0.277031          0.577747           0.618242   \n",
       "37          0.234037    0.162639          0.143730           0.018842   \n",
       "38          0.310193    0.231581          0.115093           0.026969   \n",
       "..               ...         ...               ...                ...   \n",
       "533         0.698261    0.658220          0.259462           0.347757   \n",
       "534         0.178844    0.100234          0.561332           0.343947   \n",
       "535         0.754622    0.732167          0.415413           0.443157   \n",
       "536         0.370403    0.244882          0.517556           0.379345   \n",
       "537         0.245446    0.136396          0.871409           0.461570   \n",
       "538         0.046464    0.028316          0.404469           0.077907   \n",
       "539         0.028043    0.017314          0.714546           0.431887   \n",
       "540         0.194996    0.122915          0.485636           0.281723   \n",
       "541         0.433638    0.281225          0.481076           0.612527   \n",
       "542         0.391711    0.289115          0.225718           0.163942   \n",
       "543         0.289161    0.200379          0.236662           0.164735   \n",
       "544         0.335143    0.226982          0.411765           0.268866   \n",
       "545         0.324215    0.245559          0.367989           0.186323   \n",
       "546         0.142347    0.090044          0.430917           0.085876   \n",
       "547         0.142072    0.077645          0.591427           0.302041   \n",
       "548         0.128462    0.080711          0.352485           0.097051   \n",
       "549         0.230188    0.144467          0.357045           0.204737   \n",
       "550         0.162692    0.102399          0.171911           0.062161   \n",
       "551         0.188260    0.113356          0.250342           0.228388   \n",
       "552         0.259056    0.184642          0.384405           0.114416   \n",
       "553         0.085573    0.049869          0.264934           0.077241   \n",
       "554         0.264142    0.185093          0.378021           0.202673   \n",
       "555         0.131693    0.077735          0.521204           0.216959   \n",
       "556         0.120077    0.073090          0.412677           0.136004   \n",
       "558         0.381401    0.247227          0.194710           0.448871   \n",
       "560         0.342223    0.235143          0.390789           0.304899   \n",
       "563         0.884528    0.736676          0.542180           0.609988   \n",
       "564         0.795175    0.830463          0.544916           0.280930   \n",
       "565         0.718881    0.696997          0.322389           0.250611   \n",
       "566         0.524366    0.423302          0.297766           0.436649   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "1           0.284604              0.688634        0.371706   \n",
       "2           0.530569              0.899667        0.642409   \n",
       "4           0.471198              0.601629        0.250627   \n",
       "5           0.630816              0.644576        0.759097   \n",
       "6           0.445753              0.715291        0.469887   \n",
       "7           0.315467              0.576083        0.511606   \n",
       "8           0.634939              0.762680        0.882371   \n",
       "10          0.171869              0.369308        0.433814   \n",
       "11          0.467075              0.670122        0.698557   \n",
       "13          0.273530              0.414291        0.390213   \n",
       "15          0.827659              0.633839        0.832183   \n",
       "16          0.343268              0.595705        0.459222   \n",
       "17          0.563553              0.767494        0.671581   \n",
       "18          0.632819              0.884117        0.377353   \n",
       "19          0.281541              0.476860        0.442911   \n",
       "20          0.222641              0.269641        0.507842   \n",
       "21          0.104453              0.230544        0.277604   \n",
       "22          0.742726              0.885968        0.973024   \n",
       "24          0.553069              0.775639        0.642409   \n",
       "26          0.652491              1.000000        0.846612   \n",
       "27          0.405937              0.551648        0.243413   \n",
       "28          0.746260              0.749352        0.772271   \n",
       "29          0.293203              0.539060        0.373588   \n",
       "30          0.722464              0.684191        0.589398   \n",
       "32          0.658264              0.683821        0.616374   \n",
       "33          0.717517              0.660866        0.660916   \n",
       "34          0.621275              0.690115        0.848494   \n",
       "36          0.610908              0.535728        0.635508   \n",
       "37          0.056933              0.185598        0.132371   \n",
       "38          0.028248              0.107331        0.000000   \n",
       "..               ...                   ...             ...   \n",
       "533         0.364236              0.597186        0.519134   \n",
       "534         0.250088              0.365087        0.227102   \n",
       "535         0.522205              0.795261        0.474279   \n",
       "536         0.498763              0.504258        0.355395   \n",
       "537         0.164330              0.484265        0.388331   \n",
       "538         0.000000              0.000000        0.468319   \n",
       "539         0.399694              0.185117        0.384253   \n",
       "540         0.211686              0.256127        0.239649   \n",
       "541         0.475910              0.446131        0.508783   \n",
       "542         0.189775              0.405405        0.362923   \n",
       "543         0.125103              0.294632        0.284818   \n",
       "544         0.162210              0.253425        0.214555   \n",
       "545         0.123572              0.265605        0.337829   \n",
       "546         0.051643              0.088153        0.350063   \n",
       "547         0.210037              0.308515        0.353199   \n",
       "548         0.110143              0.142392        0.309598   \n",
       "549         0.072965              0.120844        0.468632   \n",
       "550         0.000000              0.000000        0.280113   \n",
       "551         0.184238              0.237431        0.503137   \n",
       "552         0.101932              0.240578        0.264115   \n",
       "553         0.094157              0.094928        0.272898   \n",
       "554         0.287313              0.240392        0.253137   \n",
       "555         0.235599              0.337912        0.207340   \n",
       "556         0.011839              0.082636        0.218632   \n",
       "558         0.431382              0.409108        0.217378   \n",
       "560         0.156202              0.388004        0.214868   \n",
       "563         0.777359              0.941133        0.427854   \n",
       "564         0.483803              0.820437        0.155270   \n",
       "565         0.378725              0.602740        0.315872   \n",
       "566         0.400872              0.524991        0.204831   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "1                   0.429800  \n",
       "2                   0.411586  \n",
       "4                   0.274981  \n",
       "5                   0.877308  \n",
       "6                   0.362257  \n",
       "7                   0.759676  \n",
       "8                   0.659752  \n",
       "10                  0.372881  \n",
       "11                  0.629395  \n",
       "13                  0.099039  \n",
       "15                  1.000000  \n",
       "16                  0.343031  \n",
       "17                  0.748292  \n",
       "18                  0.267012  \n",
       "19                  0.221983  \n",
       "20                  0.338857  \n",
       "21                  0.286997  \n",
       "22                  0.561852  \n",
       "24                  0.513534  \n",
       "26                  0.916519  \n",
       "27                  0.242474  \n",
       "28                  0.552998  \n",
       "29                  0.305464  \n",
       "30                  0.541108  \n",
       "32                  0.376676  \n",
       "33                  0.724260  \n",
       "34                  0.863395  \n",
       "36                  0.586390  \n",
       "37                  0.084113  \n",
       "38                  0.000000  \n",
       "..                       ...  \n",
       "533                 0.111561  \n",
       "534                 0.350873  \n",
       "535                 0.261194  \n",
       "536                 0.360106  \n",
       "537                 0.564887  \n",
       "538                 0.560840  \n",
       "539                 0.652163  \n",
       "540                 0.332659  \n",
       "541                 0.597774  \n",
       "542                 0.183658  \n",
       "543                 0.118771  \n",
       "544                 0.377941  \n",
       "545                 0.183279  \n",
       "546                 0.239691  \n",
       "547                 0.502783  \n",
       "548                 0.305591  \n",
       "549                 0.268404  \n",
       "550                 0.137617  \n",
       "551                 0.319757  \n",
       "552                 0.123956  \n",
       "553                 0.238932  \n",
       "554                 0.219833  \n",
       "555                 0.351505  \n",
       "556                 0.156590  \n",
       "558                 0.316216  \n",
       "560                 0.356312  \n",
       "563                 0.552618  \n",
       "564                 0.203769  \n",
       "565                 0.143309  \n",
       "566                 0.292942  \n",
       "\n",
       "[495 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# transform data with MinMaxScaler function\n",
    "first_change[ ['radius_mean','texture_mean','perimeter_mean',\n",
    "            'area_mean','smoothness_mean','compactness_mean','concavity_mean',\n",
    "            'concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se',\n",
    "            'texture_se','perimeter_se','area_se','smoothness_se','compactness_se',\n",
    "            'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',\n",
    "            'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n",
    "            'compactness_worst','concavity_worst','concave points_worst','symmetry_worst',\n",
    "            'fractal_dimension_worst']] = scaler.fit_transform(first_change[ ['radius_mean','texture_mean','perimeter_mean',\n",
    "            'area_mean','smoothness_mean','compactness_mean','concavity_mean',\n",
    "            'concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se',\n",
    "            'texture_se','perimeter_se','area_se','smoothness_se','compactness_se',\n",
    "            'concavity_se','concave points_se','symmetry_se','fractal_dimension_se',\n",
    "            'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst',\n",
    "            'compactness_worst','concavity_worst','concave points_worst','symmetry_worst',\n",
    "            'fractal_dimension_worst']])\n",
    "\n",
    "Outlier_removed_data = first_change\n",
    "display(Outlier_removed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.980\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\tclf__penalty: 'l2'\n",
      "Accuracy: 0.991935483871\n",
      "Precision: 0.988372093023\n",
      "Recall: 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        39\n",
      "          1       0.99      1.00      0.99        85\n",
      "\n",
      "avg / total       0.99      0.99      0.99       124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report,precision_score, recall_score, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "pipeline = Pipeline(\n",
    "       [('clf', LogisticRegression())])\n",
    "parameters = {\n",
    "       'clf__penalty': ('l1', 'l2'),\n",
    "       'clf__C': (0.01, 0.1, 1, 5, 10, 15, 20, 25, 50, 75, 100),}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1', cv=3)\n",
    "    X, y = Outlier_removed_data[dataname[2:32]], Outlier_removed_data[dataname[1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print 'Best score: %0.3f' % grid_search.best_score_\n",
    "    print 'Best parameters set:'  \n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    \n",
    "    print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "    print 'Precision:', precision_score(y_test, predictions)\n",
    "    print 'Recall:', recall_score(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bb500d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "classifier = LogisticRegression(penalty='l2', C= 5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = classifier.predict_proba(X_test)\n",
    "false_positive_rate, recall, thresholds = roc_curve(y_test, predictions[:, 1])\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3249 candidates, totalling 9747 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 420 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2520 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 6020 tasks      | elapsed:   23.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.942\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 15\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 1\n",
      "Accuracy: 0.951612903226\n",
      "Precision: 0.964705882353\n",
      "Recall: 0.964705882353\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        39\n",
      "          1       0.96      0.96      0.96        85\n",
      "\n",
      "avg / total       0.95      0.95      0.95       124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 9747 out of 9747 | elapsed:   38.0s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "pipeline = Pipeline([\n",
    "           ('clf', DecisionTreeClassifier(criterion='gini'))])\n",
    "parameters = {\n",
    "           'clf__max_depth': range(1, 20),\n",
    "           'clf__min_samples_leaf': range(1, 10),\n",
    "           'clf__min_samples_split': range(1, 20)}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1', cv=3)\n",
    "    X, y = Outlier_removed_data[dataname[2:32]], Outlier_removed_data[dataname[1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print 'Best score: %0.3f' % grid_search.best_score_\n",
    "    print 'Best parameters set:'\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):        \n",
    "        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    \n",
    "    print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "    print 'Precision:', precision_score(y_test, predictions)\n",
    "    print 'Recall:', recall_score(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7290 candidates, totalling 21870 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1752 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3152 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4952 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7152 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9752 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12752 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 16152 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 19952 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 21870 out of 21870 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.974\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 6\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 4\n",
      "\tclf__n_estimators: 10\n",
      "Accuracy: 0.967741935484\n",
      "Precision: 0.965517241379\n",
      "Recall: 0.988235294118\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.92      0.95        39\n",
      "          1       0.97      0.99      0.98        85\n",
      "\n",
      "avg / total       0.97      0.97      0.97       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([('clf', RandomForestClassifier(criterion='gini'))])\n",
    "\n",
    "parameters = {\n",
    "           'clf__n_estimators': (3, 5, 10, 15, 20, 25),\n",
    "           'clf__max_depth': range(5, 20),         \n",
    "           'clf__min_samples_leaf': range(1, 10),\n",
    "           'clf__min_samples_split': range(1, 10),}\n",
    "if __name__ == '__main__':\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1', cv=3)\n",
    "    X, y = Outlier_removed_data[dataname[2:32]], Outlier_removed_data[dataname[1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print 'Best score: %0.3f' % grid_search.best_score_\n",
    "    print 'Best parameters set:'\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):        \n",
    "        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    \n",
    "    print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "    print 'Precision:', precision_score(y_test, predictions)\n",
    "    print 'Recall:', recall_score(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM rbf & linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 221 candidates, totalling 663 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 608 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 663 out of 663 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.986\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tclf__gamma: 1\n",
      "Accuracy: 0.983870967742\n",
      "Precision: 0.977011494253\n",
      "Recall: 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        39\n",
      "          1       0.98      1.00      0.99        85\n",
      "\n",
      "avg / total       0.98      0.98      0.98       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('clf', SVC(kernel='rbf'))])\n",
    "parameters = {\n",
    "           'clf__gamma': (0.00001, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 15, 20, 25, 30, 50, 75, 100),\n",
    "           'clf__C': (0.1, 0.3, 1, 3, 10, 15, 20, 25, 30, 50, 75, 100, 150)}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs= -1 ,verbose=1, scoring='f1', cv =3)\n",
    "    X, y = Outlier_removed_data[dataname[2:32]], Outlier_removed_data[dataname[1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print 'Best score: %0.3f' % grid_search.best_score_\n",
    "    print 'Best parameters set:'\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "    print 'Precision:', precision_score(y_test, predictions)\n",
    "    print 'Recall:', recall_score(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 13 candidates, totalling 39 fits\n",
      "Best score: 0.982\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "Accuracy: 0.991935483871\n",
      "Precision: 0.988372093023\n",
      "Recall: 1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        39\n",
      "          1       0.99      1.00      0.99        85\n",
      "\n",
      "avg / total       0.99      0.99      0.99       124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "    \n",
    "pipeline = Pipeline([('clf', SVC(kernel='linear'))])\n",
    "parameters = {\n",
    "           \n",
    "           'clf__C': (0.1, 0.3, 1, 3, 10, 15, 20, 25, 30, 50, 75, 100, 150)}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs= -1,verbose=1, scoring='f1', cv = 3)\n",
    "    X, y = Outlier_removed_data[dataname[2:32]], Outlier_removed_data[dataname[1]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print 'Best score: %0.3f' % grid_search.best_score_\n",
    "    print 'Best parameters set:'\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "    print 'Precision:', precision_score(y_test, predictions)\n",
    "    print 'Recall:', recall_score(y_test, predictions)\n",
    "    print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
